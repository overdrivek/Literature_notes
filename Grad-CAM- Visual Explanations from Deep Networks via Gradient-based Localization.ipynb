{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient-weighted Class Activation Mapping (Grad-CAM) uses the gradients of any target concept (say logits for 'dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of image classification models, the visualizations \n",
    "1. lend insights into the failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), \n",
    "2. are robust to adversarial images, \n",
    "3. outperform previous methods on the ILSVRC-15 weakly-supervised localization task, \n",
    "4. are more faithful to the underlying model, and \n",
    "5. help achieve model generalization by identifying dataset bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# github: https://github.com/ramprs/grad-cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper: https://arxiv.org/pdf/1610.02391.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grad-CAM is a class-discriminative localization technique that can generate visual explanations from any CNN-based network without requiring architectural changes or retraining. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Grad-CAM uses the gradient information floying into the last convolutional layer of the CNN to understand the importance of each neuron for a decision of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://cdn-images-1.medium.com/max/1600/1*KSTv_NEaeQ1cj_mrEnFXrw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain the class-discriminative localization map Grad-CAM $L^c_{Grad-CAM} \\epsilon R^{u\\times v}$ of width $u$ and height $v$ for any class $c$, we first compute the gradient of the score for class $c,y^c$(before the softmax), with respect to the feature maps $A^k$ of a convolutional layer, i.e. $\\frac{\\partial y^c}{\\partial A^k_{ij}}$. These gradients flowing back are global-average-pooled to obtain the neuron importance weights $\\alpha^c_k$. \n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha^c_k=\\frac{1}{Z}\\Sigma_i\\Sigma_j \\frac{\\partial y^c}{\\partial A^k_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "Perform a weighted combination of forward activation maps, and follow it by a ReLU to obtain, \n",
    "\n",
    "\\begin{equation}\n",
    "L^c_{Grad-CAM} = ReLU(\\Sigma_k \\alpha^c_k A^k )\n",
    "\\end{equation}. \n",
    "This results in a coarse heat-map of the same size as the convolutional feature maps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Grad-CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method fuses Guided Backpropagation and Grad-CAM visualizations via point-wise multiplication ($L^c_{Grad-CAM}$ is first up-sampled to the input image resolution using bi-linear interpolation. Check image above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a new explanation modality. Using a slight modification to Grad-CAM, we obtain these counterfactural explanations, which highlight the support for the regions that would make the network change its decision. Removing concepts occurring in those regions would make the model more confident about the given target decision. \n",
    "Specifically, we negate the gradient of $y^c$ (score for class $c$) with respect to the feature maps $A$ of a convolutional layer. Thus the importance weights $\\alpha^c_k$, now become, \n",
    "\\begin{equation}\n",
    "\\alpha^c_k=\\frac{1}{Z}\\Sigma_i\\Sigma_j -\\frac{\\partial y^c}{\\partial A^k_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "We weighted sum the forward activation maps, A with weights $\\alpha^c_k}, and follow it by a ReLU to obtain counter factural explanations as shown in the figure below, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://hugrypiggykim.com/wp-content/uploads/2018/03/gradcam-6.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
