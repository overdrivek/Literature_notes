{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias vs Variance\n",
    "Two kinds of errors: \"Reducible Errors\" and \"Irreducible Errors\"\n",
    "Irreducible Errors are the errors that arise from causes inherent to the system. \n",
    "Reducible Errors arise because of either \"error due to squared bias\" or \"error due to variance\". The goal is to reduce both the bias and variance as much as possible and also acquiring high accuracy and generalization models. \n",
    "\n",
    "Error due to Squared bias is the amount by which the model prediction differs from the true value or target, over the training data. If the prediction values are substantially different from the true value, the model bias is high. Typically arises where the training data are all of one kind and the model knows only one kind of data and shows high error for slightly different set of data. \n",
    "\n",
    "\n",
    "Error due to variance is the amount by which the prediction, over one training set, differs from the expected prediction value over all training sets. Variance predicts the inconsistence of the predictions from one another. \n",
    "\n",
    "* Models with high bias and low variance underfit the truth target. \n",
    "* Models with low bias and high variance overfit the truth target. \n",
    "\n",
    "The tradeoff can be achieved by having more varied training data and using the right metric. Cross-validation is one way to identify and achieve this tradeoff. \n",
    "\n",
    "Ref: https://insidebigdata.com/2014/10/22/ask-data-scientist-bias-vs-variance-tradeoff/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall\n",
    "![title](https://cdn-images-1.medium.com/max/1600/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n",
    "Precision tells how many correct instances for a class are predicted among all the prediction for the class. In other words \"How many selected items are relevant?\"\n",
    "\n",
    "$\\text{precision} = \\frac{tp}{tp+fp} = \\frac{\\text{true positive}}{\\text{Total predicted positive}}$\n",
    "\n",
    "           \n",
    "\n",
    "Recall tells the precentage of relevant items predicted from the actual list of relevant items. In other words, \"How many relevant items are selected?\"\n",
    "\n",
    "$\\text{recall}=\\text{sensitivity} = \\frac{tp}{tp+fn}=\\frac{\\text{true positive}}{\\text{Total actual positive}}$\n",
    "\n",
    "$\\text{specificity} = \\frac{tn}{fn+fp}$\n",
    "\n",
    "Specificity tells how good a test is at avoiding false alams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score\n",
    "F1 score is a measure of a test's accuracy. It considers both precision $p$ and recall $r$ of the test to compute the score. \n",
    "F1 score is given by, \n",
    "\\begin{equation}\n",
    "F1 = 2.\\frac{\\text{precision}.\\text{recall}}{\\text{precision}+\\text{recall}}\n",
    "\\end{equation}\n",
    "\n",
    "F1 is the harmonic mean of precision and recall. In the event of unbalanced datasets F1 score might be a better measure to use if one needs to seek a balance between Precision and Recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harmonic mean\n",
    "The harmonic mean (sometimes called as the subcontrary mean) is one of the several kinds of average, and in particular one of the Pythagorean means. Typically, it is appropriate for situations when the average of *rates* is desired. \n",
    "The harmonic mean $H$ of the positive real numbers $x_1,x_2,....,x_n$ is defined to be \n",
    "\\begin{equation}\n",
    "H = \\frac{n}{\\frac{1}{x_1}+\\frac{1}{x_2}+......+\\frac{1}{x_n}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type I and Type II Errors\n",
    "Type I Error: \n",
    "* Rejecting Null Hypothesis but it is *True*.\n",
    "Type II Error: \n",
    "* Null Hypothesis if *False* and it is *not rejected*. Potentially high risk. \n",
    "\n",
    "False Positives FP --> Type I Error\n",
    "False Negative FN --> Type II Error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC and ROC Curves\n",
    "AUC - Area under the curve\n",
    "ROC - Receive Operator Characteristic \n",
    "ROC curve is a performance measurement for classification problem at various threshold settings. It the curve with false positive rate in x-axis and true positive rate in the y-axis.  \n",
    "\n",
    "True Positive Rate is RECALL. \n",
    "\n",
    "For an excellent model, the AUC has to be closer to 1. \n",
    "AUC gives a measure of separability between good and bad samples. \n",
    "\n",
    "For a multi-class model, one can use one vs All methodology to get many ROC curves. \n",
    "\n",
    "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-supervised learning\n",
    "Semi-supervised learning is class of ML algorithm which fallse between supervised and unsupervised learning techniques. This is because, the dataset consists of a small amount of labeled and a larget quantity of unlabeled data. \n",
    "\n",
    "### Self-supervised learning: \n",
    "Refers to the approach where the labels can be generated automatically. \n",
    "Eg. Autoencoders\n",
    "\n",
    "### Weakly supervised learning: \n",
    "Bootstrapping, also called self-training, is a form of learning that is designed to use even less training examples, therefore sometimes called weakly-supervised. Bootstrapping starts with a few training examples, trains a classifier, and uses thought-to-be positive examples as yielded by this classifier for retraining. As the set of training examples grows, the classifier improves, provided that not too many negative examples are misclassified as positive, which could lead to deterioration of performance.\n",
    "\n",
    "\n",
    "Ref: https://stackoverflow.com/questions/18944805/what-is-weakly-supervised-learning-bootstrapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours vs K-Means\n",
    "K-means is a clustering algorithm that tries to partition a set of points into K sets (clusters) such that the points in each cluster tend to be near each other. It is unsupervised because the points have no external classification.\n",
    "\n",
    "K-nearest neighbors is a classification (or regression) algorithm that in order to determine the classification of a point, combines the classification of the K nearest points. It is supervised because you are trying to classify a point based on the known classification of other points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "It is a classification methodology which uses Bayes Theorem with an assumption that the individual input features are independent to each other and bear no cross-correlation. This basic assumption is the reason why naive bayes is called \"naive\". \n",
    "\n",
    "Bayes theorem says \n",
    "\\begin{align}\n",
    "P(c|x) &= \\frac{P(x|c).P(c)}{P(x)} \\\\\n",
    "\\text{Posterior Probability} &= \\frac{\\text{Likelihood.Prior probability class}}{\\text{Prior probability prediction}}\n",
    "\\end{align}\n",
    "\n",
    "##### Pros:\n",
    "* Easy and fast to predict class of test data set, \n",
    "* If the assumption of independence holds, performs even better than other classification methods, \n",
    "* performs well for categorical variables compared to numerical variables. For numerical variable, normal distribution is assumed. \n",
    "\n",
    "##### Cons: \n",
    "* Has problem generalizing against novel examples.\n",
    "* In real life, independent variables do not occur. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression / Classification Trees (Decision Trees)\n",
    "Decision trees is one machine learning algorithm used for both classification and regression. The algorithm works by creating a tree with individual feature decicisions at the different nodes of the tree. The tree starts with the root node at the top. The tree is split down to different branches and the end of the branch which doesnt split is called a leaf. \n",
    "\n",
    "The tree creation starts by finding out all different feature splits and computing the accuracy or the cost. The split that costs the least is chosen. The algorithm then works in recursive manner as the groups formed are further subdivided using the same strategy. This is one of the reason for naming the algorithm also a \"Greedy algorithm\". \n",
    "\n",
    "Typical cost of a split is the mean squared error or the sum of squared error for regression. \n",
    "\n",
    "The cost used for classification is the gini index given by, \n",
    "\\begin{equation}\n",
    "G = \\sum{p_k*(1-p_k)}\n",
    "\\end{equation}\n",
    "\n",
    "Termination criteria could be set by defining, \n",
    "* Minimum number of training inputs to use on each leaf\n",
    "* Maximum depth (length of the longest branch)\n",
    "\n",
    "One can also do pruning to remove unwanted branches and features that contain superfluous information. \n",
    "\n",
    "Advantages:\n",
    "* Fast\n",
    "* Easy to interpret\n",
    "* Feature selection\n",
    "* Non-linear relationship between parameters do not affect tree performance. \n",
    "\n",
    "Disadvantages: \n",
    "* Can create complex trees which may lead to overfitting. \n",
    "* Unstable to variations in data. Generalization problem.\n",
    "* Greedy algorithms cannot guarantee global optimality. \n",
    "* Can create biased trees if some classes dominate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini Index\n",
    "Gini index says, if we select two items from a population at random if they are from the same class. If the probability is 1 it would mean the population is pure. \n",
    "The Gini impurity is thus measured by computing the probability $p_i$ of an item with label $i$ being chosen times the probability $\\sum_{k\\neq i}p_k=1-p_i$ of a mistake in categorizing that item. It reaches its minimum when all cases in the node fall into a single target category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "Random forest is a supervised learning algorithms which uses an ensemble of decision trees which is termed as forest for classification. The training is performed using bagging technique. Random forest also provides a nice platform to analyse feature importance. This is done by removing the feature nodes from all the trees and computing the fall in accuracy. This is done for all features across all trees and at every iteration to compute the feature importance. \n",
    "Important hyper-parameters are \n",
    "* number of estimators: number of trees the algorithm needs to build.\n",
    "* maximum features: maximum features random forest uses in a tree\n",
    "* minimum sample leaf: minimum number of leafs that are required to split an internal node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 and L2 Loss\n",
    "L1-norm loss is also known as mean absolute deviation or least absolute errors. It is basically minimizing the sum of absolute differences between the target $y_i$ and the prediction $f(x_i)$. \n",
    "\\begin{equation}\n",
    "L1_loss = \\sum_{i=1}^{n}|y_i-f(x_i)|\n",
    "\\end{equation}\n",
    "\n",
    "L2-norm loss is also known as least squares error. It is basically minimizing the sum of the square of the differences between the target $y_i$ and the prediction $f(x_i)$\n",
    "\\begin{equation}\n",
    "L2_loss = \\sum_{i=1}^{n}(y_i-f(x_i))^2 \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 and L2 Regularization\n",
    "Regularization is a technique in machine learning to prevent overfitting. Mathematically speaking, it adds a regularization term in order to prevent the coefficients to fit so perfectly. \n",
    "L1 regularization on least squares \n",
    "\\begin{equation}\n",
    "w^* = \\text{arg}\\min\\sum_j(t(x_j)-\\sum_i w_ih_i(x_j))^2 + \\lambda \\sum_{i=1}^{k}|w_i|\n",
    "\\end{equation}\n",
    "L2 regularization on least squares\n",
    "\\begin{equation}\n",
    "w^* = \\text{arg}\\min\\sum_j(t(x_j)-\\sum_i w_ih_i(x_j))^2 + \\lambda \\sum_{i=1}^{k}w_i^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "Gradient Descent is an optimization algorithm to update the weights to reduce a given cost function. Starting from an initial value, gradient descent iteratively adapts the weight to reduce the cost function. The cost function can be a non-convex function or a convex function. Non-convex functions can have many local local minimas. With linear regression it is always a convex function. \n",
    "Example: \n",
    "If the cost function $J(.)$, the sum of squared errors (SSE) written as , \n",
    "\\begin{equation}\n",
    "J(w) = \\frac{1}{2}\\sum_i(target^{(i)}-output^{(i)})^2\n",
    "\\end{equation}\n",
    "the magnitude and direction of the weight update is computed by a step in the opposite direction of the cost gradient\n",
    "\\begin{equation}\n",
    "\\Delta w_j = -\\eta \\frac{\\partial J}{\\partial w_j}\n",
    "\\end{equation}\n",
    "where $\\eta$ is the learning rate. The weights are then updated after each *epoch* via, \n",
    "\\begin{equation}\n",
    "w := w + \\Delta w\n",
    "\\end{equation}\n",
    "where $\\Delta w$ is a vector that contains the weight updates of each weight coefficient $w$, which are computed as follows:\n",
    "\\begin{align}\n",
    "\\Delta w_j &= -\\eta \\frac{\\partial J}{\\partial w_j}\\\\\n",
    "&= -\\eta \\sum_i (target^{(i)} - output^{(i)}) (-x_j^{(i)})\\\\\n",
    "&= \\eta \\sum_i (target^{(i)} - output^{(i)}) (-x_j^{(i)})\n",
    "\\end{align}\n",
    "In GD optimization, we compute the cost gradient based on the complete training set; hence, it sometimes is called *batch GD*. In case of very large datasets, using GD can be quite costly since we are only taking a single step for one pass over the training set -- thus, the larger the training set, the slower our weight updates and longer it takes to converge to the global cost minimum. \n",
    "\n",
    "* for one or more epochs:\n",
    "    * for each weight $j$\n",
    "        * $w_j := w + \\Delta w_j$, where: $\\Delta w_j = \\eta \\sum_i (target^{(i)} - output^{(i)}) x_j^{(i)}$\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "In Gradient descent, in order to update the weights one needs to loop through all the samples to update the weights on one epoch. This is extremely time consuming. Stochastic gradient descent does the weight update on only one sample and not all the data. The weight updates are random and noisy but it takes less time. It is important thus to shuffle the data before training. If there are n batches, then there are n updates on n samples every epoch. \n",
    "\n",
    "* for one or more epochs, or until approx. cost minimum is reached:\n",
    "    * for training sample $i$\n",
    "        * for each weight $j$\n",
    "            * $w_j := w + \\Delta w_j$, where: $\\Delta w_j = \\eta \\sum_i (target^{(i)} - output^{(i)}) x_j^{(i)}$\n",
    "\n",
    "The term *stochastic* comes from the fact that the gradient based on a single trainign sample is a \"stochastic approximation\" of the \"true\" cost gradient. Due to its stochastic nature, the path towards the global cost minimum is not \"direct\" as in GD, but may go \"zig-zag\" if we are visualizing the cost surface in a 2D space. However, it has been shown that SGD almost surely converges to the global cost minimum if the cost function is convex. \n",
    "\n",
    "#### Some tricks\n",
    "* An **adaptive learning rate** $\\eta$. Choosing a decrease constant $d$ that shrinks the learning rate over time: \n",
    "\n",
    "    $\\eta(t+1) := \\eta(t)/(1+t.d)$\n",
    "    \n",
    "    \n",
    "* **Momentum learning** by adding a factor of the previous gradient to the weight update for faster updates:\n",
    "\n",
    "    $\\Delta w_{t+1} := \\eta \\Delta J(w_{t+1})+\\alpha \\Delta w_t$\n",
    "    \n",
    "    \n",
    "* **Nesterov accelerated gradient** is an update of momentum learning where not only the previous gradient of the parameter is added, but also the direction of the parameter is also considered. This gives an approximation of the next position of the parameters, a rough idea where our parameters are going to be. \n",
    "\n",
    "    $\\Delta w_{t+1} := \\eta \\Delta J(w_{t+1}-w_t) + \\alpha \\Delta w_t$\n",
    "    \n",
    "![title](http://ruder.io/content/images/2016/09/nesterov_update_vector.png)\n",
    "http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
    "\n",
    "While Momentum first computes the current gradient (small blue vector in Image) and then takes a big jump in the direction of the updated accumulated gradient (big blue vector), NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (red vector), which results in the complete NAG update (green vector). This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks\n",
    "\n",
    "\n",
    "There are several different flavors of SGD, which can be all seen throughout the literature. Let's take a look at the three most common variants:\n",
    "\n",
    "(A)\n",
    "\n",
    "* randomly shuffle samples in the training set\n",
    "    * for one or more epochs, or until approx. cost minimum is reached\n",
    "        * for training sample $i$\n",
    "            * compute gradients and perform weight updates\n",
    "\n",
    "(B)\n",
    "\n",
    "* for one or more epochs, or until approx. cost minimum is reached\n",
    "    * randomly shuffle samples in the training set\n",
    "        * for training sample $i$\n",
    "            * compute gradients and perform weight updates\n",
    "\n",
    "(C)\n",
    "\n",
    "* for iterations $t$, or until approx. cost minimu is reached:\n",
    "    * draw random sample from the training set\n",
    "        * compute gradients and perform weight updates\n",
    "        \n",
    "In scenario A, we shuffle the training set only one time in the beginning; whereas in scenario B, we shuffle the training set after each epoch to prevent repeating update cycles. In both scenario A and scenario B, each training sample is only used once per epoch to update the model weights.\n",
    "In scenario C, we draw the training samples randomly with replacement from the training set. If the number of iterations t is equal to the number of training samples, we learn the model based on a bootstrap sample of the training set.\n",
    "\n",
    "#### MINI-BATCH GRADIENT DESCENT (MB-GD)\n",
    "\n",
    "Mini-Batch Gradient Descent (MB-GD) a compromise between batch GD and SGD. In MB-GD, we update the model based on smaller groups of training samples; instead of computing the gradient from 1 sample (SGD) or all n training samples (GD), we compute the gradient from 1 < k < n training samples (a common mini-batch size is k=50).\n",
    "MB-GD converges in fewer iterations than GD because we update the weights more frequently; however, MB-GD let's us utilize vectorized operation, which typically results in a computational performance gain over SGD.\n",
    "\n",
    "https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adagrad\n",
    "\n",
    "Adagrad is an algorithm for gradient based optimization that adapts the learning rate to the parameters, performing smaller updates for parameters associated with frequently occuring features, and larger updates for parameters with sparsely occuring features. For this reason, it is well suited for dealing with sparse data.\n",
    "\n",
    "The SGD update for every parameter is \n",
    "\n",
    "\\begin{align}\n",
    "w_{t+1,i} &= w_{t,i} - \\eta \\Delta J(w_{t}) \\\\\n",
    "&= w_{t,i}-\\eta g_{t,i}\n",
    "\\end{align}\n",
    "where $g_{t,i}=\\Delta J(w_t)$. \n",
    "\n",
    "Adagrad modifies the learning rate $\\eta$ at each time step $t$ for every parameter $w_i$ based on the past gradients that have been computed for $w_i$:\n",
    "\\begin{equation}\n",
    "w_{t+1,i} = w_{t,i} - \\frac{\\eta}{\\sqrt{G_{t,ii}+\\epsilon}}.g_{t,i}\n",
    "\\end{equation}\n",
    "where $G_{t}$ is a diagonal matrix where each diagonal element $i,i$ is the sum of the squares of the gradients w.r.t. $w_i$ up to time step $t$, while $\\epsilon$ is a smoothing term that avoids division by zero. \n",
    "One of the main advantages of Adagrad is that it eliminates the need to manually tune the learning rate. Adagrads main weakness is its accumulation of the squared gradients in the denominator: since every added term is positive, the accumulated sum keeps growing making the learning rate to shrink and eventually become infinitesimally small. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adadelta\n",
    "Adadelta is an extention of Adagrad that seeks to reduce the monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size window $w$. \n",
    "The parameter update vector of Adadelta thus becomes \n",
    "\\begin{equation}\n",
    "w_{t+1,i} = w_{t,i} - \\frac{\\eta}{\\sqrt{E[g^2]_t+\\epsilon}}.g_t\n",
    "\\end{equation}\n",
    "\n",
    "where $E[g^2]_t$ is the decaying average over the past squared gradients. \n",
    "The denominator is now just the root mean squared (RMS) error criterion of the gradient, so the equation becomes, \n",
    "\\begin{equation}\n",
    "w_{t+1,i} = w_{t,i} - \\frac{\\eta}{RMS[g]_t}.g_t\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSProp\n",
    "RMSprop is in fact identical to the first update vector of Adadelta, \n",
    "\\begin{align}\n",
    "E[g^2]_t &= 0.9E[g^2]_{t-1}+0.1g_t^2 \\\\\n",
    "w_{t+1} &= w_t - \\frac{\\eta}{\\sqrt{E[g^2]_t+\\epsilon}}.g_t\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam (Adaptive Moment Estimation)\n",
    "In addition to storing an exponentially decaying average of past squared gradients like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of the past gradients, similar to momentum. \n",
    "We compute the decaying averages of past and past squared gradients $m_t$ and $v_t$ respectively as follows, \n",
    "\\begin{align}\n",
    "m_t &= \\beta_1 m_{t-1} + (1-\\beta_1 )g_t \\\\\n",
    "v_t &= \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2\n",
    "\\end{align}\n",
    "where, $m_t$ and $v_t$ are estimates of the first moment (mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As $m_t$ and $v_t$ are initialized as vectors of 0's, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small. \n",
    "They counteract these biases by computing bias-corrected first and second moment estimates \n",
    "\\begin{align}\n",
    "\\hat m_t &= \\frac{m_t}{1-\\beta_1^t} \\\\\n",
    "\\hat v_t &= \\frac{v_t}{1-\\beta_2^t}\n",
    "\\end{align}\n",
    "They then use these to update the parameters just as in Adadelta and RMSprop, which yields the Adam update rule: \n",
    "\\begin{equation}\n",
    "w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\hat v_t}+\\epsilon}.\\hat m_t\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore there are Adamax, Nadam (Nesterov accelerated ADAM)\n",
    "check: http://ruder.io/optimizing-gradient-descent/index.html#momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://ruder.io/content/images/2016/09/contours_evaluation_optimizers.gif)\n",
    "![title](http://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which optimizer to use?\n",
    "So, which optimizer should you now use? If your input data is sparse, then you likely achieve the best results using one of the adaptive learning-rate methods. An additional benefit is that you won't need to tune the learning rate but likely achieve the best results with the default value.\n",
    "\n",
    "In summary, RMSprop is an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta, except that Adadelta uses the RMS of parameter updates in the numinator update rule. Adam, finally, adds bias-correction and momentum to RMSprop. Insofar, RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar circumstances. Kingma et al.  show that its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar, Adam might be the best overall choice.\n",
    "\n",
    "Interestingly, many recent papers use vanilla SGD without momentum and a simple learning rate annealing schedule. As has been shown, SGD usually achieves to find a minimum, but it might take significantly longer than with some of the optimizers, is much more reliant on a robust initialization and annealing schedule, and may get stuck in saddle points rather than local minima. Consequently, if you care about fast convergence and train a deep or complex neural network, you should choose one of the adaptive learning rate methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization\n",
    "To facilitate learning, we typically normalize the initial values of our parameters by initializing them with zero mean and unit variance. As training progresses and we update parameters to different extents, we lose this normalization, which slows down training and amplifies changes as the network becomes deeper.\n",
    "\n",
    "Batch normalization reestablishes these normalizations for every mini-batch and changes are back-propagated through the operation as well. By making normalization part of the model architecture, we are able to use higher learning rates and pay less attention to the initialization parameters. Batch normalization additionally acts as a regularizer, reducing (and sometimes even eliminating) the need for Dropout."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
