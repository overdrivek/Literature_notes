{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias vs Variance\n",
    "Two kinds of errors: \"Reducible Errors\" and \"Irreducible Errors\"\n",
    "Irreducible Errors are the errors that arise from causes inherent to the system. \n",
    "Reducible Errors arise because of either \"error due to squared bias\" or \"error due to variance\". The goal is to reduce both the bias and variance as much as possible and also acquiring high accuracy and generalization models. \n",
    "\n",
    "Error due to Squared bias is the amount by which the model prediction differs from the true value or target, over the training data. If the prediction values are substantially different from the true value, the model bias is high. Typically arises where the training data are all of one kind and the model knows only one kind of data and shows high error for slightly different set of data. \n",
    "\n",
    "\n",
    "Error due to variance is the amount by which the prediction, over one training set, differs from the expected prediction value over all training sets. Variance predicts the inconsistence of the predictions from one another. \n",
    "\n",
    "* Models with high bias and low variance underfit the truth target. \n",
    "* Models with low bias and high variance overfit the truth target. \n",
    "\n",
    "The tradeoff can be achieved by having more varied training data and using the right metric. Cross-validation is one way to identify and achieve this tradeoff. \n",
    "\n",
    "Ref: https://insidebigdata.com/2014/10/22/ask-data-scientist-bias-vs-variance-tradeoff/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall\n",
    "![title](https://cdn-images-1.medium.com/max/1600/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n",
    "Precision tells how many correct instances for a class are predicted among all the prediction for the class. In other words \"How many selected items are relevant?\"\n",
    "\n",
    "$\\text{precision} = \\frac{tp}{tp+fp} = \\frac{\\text{true positive}}{\\text{Total predicted positive}}$\n",
    "\n",
    "           \n",
    "\n",
    "Recall tells the precentage of relevant items predicted from the actual list of relevant items. In other words, \"How many relevant items are selected?\"\n",
    "\n",
    "$\\text{recall}=\\text{sensitivity} = \\frac{tp}{tp+fn}=\\frac{\\text{true positive}}{\\text{Total actual positive}}$\n",
    "\n",
    "$\\text{specificity} = \\frac{tn}{fn+fp}$\n",
    "\n",
    "Specificity tells how good a test is at avoiding false alams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score\n",
    "F1 score is a measure of a test's accuracy. It considers both precision $p$ and recall $r$ of the test to compute the score. \n",
    "F1 score is given by, \n",
    "\\begin{equation}\n",
    "F1 = 2.\\frac{\\text{precision}.\\text{recall}}{\\text{precision}+\\text{recall}}\n",
    "\\end{equation}\n",
    "\n",
    "F1 is the harmonic mean of precision and recall. In the event of unbalanced datasets F1 score might be a better measure to use if one needs to seek a balance between Precision and Recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harmonic mean\n",
    "The harmonic mean (sometimes called as the subcontrary mean) is one of the several kinds of average, and in particular one of the Pythagorean means. Typically, it is appropriate for situations when the average of *rates* is desired. \n",
    "The harmonic mean $H$ of the positive real numbers $x_1,x_2,....,x_n$ is defined to be \n",
    "\\begin{equation}\n",
    "H = \\frac{n}{\\frac{1}{x_1}+\\frac{1}{x_2}+......+\\frac{1}{x_n}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type I and Type II Errors\n",
    "Type I Error: \n",
    "* Rejecting Null Hypothesis but it is *True*.\n",
    "Type II Error: \n",
    "* Null Hypothesis if *False* and it is *not rejected*. Potentially high risk. \n",
    "\n",
    "False Positives FP --> Type I Error\n",
    "False Negative FN --> Type II Error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC and ROC Curves\n",
    "AUC - Area under the curve\n",
    "ROC - Receive Operator Characteristic \n",
    "ROC curve is a performance measurement for classification problem at various threshold settings. It the curve with false positive rate in x-axis and true positive rate in the y-axis.  \n",
    "\n",
    "True Positive Rate is RECALL. \n",
    "\n",
    "For an excellent model, the AUC has to be closer to 1. \n",
    "AUC gives a measure of separability between good and bad samples. \n",
    "\n",
    "For a multi-class model, one can use one vs All methodology to get many ROC curves. \n",
    "\n",
    "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-supervised learning\n",
    "Semi-supervised learning is class of ML algorithm which fallse between supervised and unsupervised learning techniques. This is because, the dataset consists of a small amount of labeled and a larget quantity of unlabeled data. \n",
    "\n",
    "### Self-supervised learning: \n",
    "Refers to the approach where the labels can be generated automatically. \n",
    "Eg. Autoencoders\n",
    "\n",
    "### Weakly supervised learning: \n",
    "Bootstrapping, also called self-training, is a form of learning that is designed to use even less training examples, therefore sometimes called weakly-supervised. Bootstrapping starts with a few training examples, trains a classifier, and uses thought-to-be positive examples as yielded by this classifier for retraining. As the set of training examples grows, the classifier improves, provided that not too many negative examples are misclassified as positive, which could lead to deterioration of performance.\n",
    "\n",
    "\n",
    "Ref: https://stackoverflow.com/questions/18944805/what-is-weakly-supervised-learning-bootstrapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours vs K-Means\n",
    "K-means is a clustering algorithm that tries to partition a set of points into K sets (clusters) such that the points in each cluster tend to be near each other. It is unsupervised because the points have no external classification.\n",
    "\n",
    "K-nearest neighbors is a classification (or regression) algorithm that in order to determine the classification of a point, combines the classification of the K nearest points. It is supervised because you are trying to classify a point based on the known classification of other points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "It is a classification methodology which uses Bayes Theorem with an assumption that the individual input features are independent to each other and bear no cross-correlation. This basic assumption is the reason why naive bayes is called \"naive\". \n",
    "\n",
    "Bayes theorem says \n",
    "\\begin{align}\n",
    "P(c|x) &= \\frac{P(x|c).P(c)}{P(x)} \\\\\n",
    "\\text{Posterior Probability} &= \\frac{\\text{Likelihood.Prior probability class}}{\\text{Prior probability prediction}}\n",
    "\\end{align}\n",
    "\n",
    "##### Pros:\n",
    "* Easy and fast to predict class of test data set, \n",
    "* If the assumption of independence holds, performs even better than other classification methods, \n",
    "* performs well for categorical variables compared to numerical variables. For numerical variable, normal distribution is assumed. \n",
    "\n",
    "##### Cons: \n",
    "* Has problem generalizing against novel examples.\n",
    "* In real life, independent variables do not occur. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression / Classification Trees (Decision Trees)\n",
    "Decision trees is one machine learning algorithm used for both classification and regression. The algorithm works by creating a tree with individual feature decicisions at the different nodes of the tree. The tree starts with the root node at the top. The tree is split down to different branches and the end of the branch which doesnt split is called a leaf. \n",
    "\n",
    "The tree creation starts by finding out all different feature splits and computing the accuracy or the cost. The split that costs the least is chosen. The algorithm then works in recursive manner as the groups formed are further subdivided using the same strategy. This is one of the reason for naming the algorithm also a \"Greedy algorithm\". \n",
    "\n",
    "Typical cost of a split is the mean squared error or the sum of squared error for regression. \n",
    "\n",
    "The cost used for classification is the gini index given by, \n",
    "\\begin{equation}\n",
    "G = \\sum{p_k*(1-p_k)}\n",
    "\\end{equation}\n",
    "\n",
    "Termination criteria could be set by defining, \n",
    "* Minimum number of training inputs to use on each leaf\n",
    "* Maximum depth (length of the longest branch)\n",
    "\n",
    "One can also do pruning to remove unwanted branches and features that contain superfluous information. \n",
    "\n",
    "Advantages:\n",
    "* Fast\n",
    "* Easy to interpret\n",
    "* Feature selection\n",
    "* Non-linear relationship between parameters do not affect tree performance. \n",
    "\n",
    "Disadvantages: \n",
    "* Can create complex trees which may lead to overfitting. \n",
    "* Unstable to variations in data. Generalization problem.\n",
    "* Greedy algorithms cannot guarantee global optimality. \n",
    "* Can create biased trees if some classes dominate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini Index\n",
    "Gini index says, if we select two items from a population at random if they are from the same class. If the probability is 1 it would mean the population is pure. \n",
    "The Gini impurity is thus measured by computing the probability $p_i$ of an item with label $i$ being chosen times the probability $\\sum_{k\\neq i}p_k=1-p_i$ of a mistake in categorizing that item. It reaches its minimum when all cases in the node fall into a single target category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "Random forest is a supervised learning algorithms which uses an ensemble of decision trees which is termed as forest for classification. The training is performed using bagging technique. Random forest also provides a nice platform to analyse feature importance. This is done by removing the feature nodes from all the trees and computing the fall in accuracy. This is done for all features across all trees and at every iteration to compute the feature importance. \n",
    "Important hyper-parameters are \n",
    "* number of estimators: number of trees the algorithm needs to build.\n",
    "* maximum features: maximum features random forest uses in a tree\n",
    "* minimum sample leaf: minimum number of leafs that are required to split an internal node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 and L2 Loss\n",
    "L1-norm loss is also known as mean absolute deviation or least absolute errors. It is basically minimizing the sum of absolute differences between the target $y_i$ and the prediction $f(x_i)$. \n",
    "\\begin{equation}\n",
    "L1_loss = \\sum_{i=1}^{n}|y_i-f(x_i)|\n",
    "\\end{equation}\n",
    "\n",
    "L2-norm loss is also known as least squares error. It is basically minimizing the sum of the square of the differences between the target $y_i$ and the prediction $f(x_i)$\n",
    "\\begin{equation}\n",
    "L2_loss = \\sum_{i=1}^{n}(y_i-f(x_i))^2 \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 and L2 Regularization\n",
    "Regularization is a technique in machine learning to prevent overfitting. Mathematically speaking, it adds a regularization term in order to prevent the coefficients to fit so perfectly. \n",
    "L1 regularization on least squares \n",
    "\\begin{equation}\n",
    "w^* = \\text{arg}\\min\\sum_j(t(x_j)-\\sum_i w_ih_i(x_j))^2 + \\lambda \\sum_{i=1}^{k}|w_i|\n",
    "\\end{equation}\n",
    "L2 regularization on least squares\n",
    "\\begin{equation}\n",
    "w^* = \\text{arg}\\min\\sum_j(t(x_j)-\\sum_i w_ih_i(x_j))^2 + \\lambda \\sum_{i=1}^{k}w_i^2\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
