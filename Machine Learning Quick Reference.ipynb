{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias vs Variance\n",
    "Two kinds of errors: \"Reducible Errors\" and \"Irreducible Errors\"\n",
    "Irreducible Errors are the errors that arise from causes inherent to the system. \n",
    "Reducible Errors arise because of either \"error due to squared bias\" or \"error due to variance\". The goal is to reduce both the bias and variance as much as possible and also acquiring high accuracy and generalization models. \n",
    "\n",
    "Error due to Squared bias is the amount by which the model prediction differs from the true value or target, over the training data. If the prediction values are substantially different from the true value, the model bias is high. Typically arises where the training data are all of one kind and the model knows only one kind of data and shows high error for slightly different set of data. \n",
    "\n",
    "\n",
    "Error due to variance is the amount by which the prediction, over one training set, differs from the expected prediction value over all training sets. Variance predicts the inconsistence of the predictions from one another. \n",
    "\n",
    "* Models with high bias and low variance underfit the truth target. \n",
    "* Models with low bias and high variance overfit the truth target. \n",
    "\n",
    "The tradeoff can be achieved by having more varied training data and using the right metric. Cross-validation is one way to identify and achieve this tradeoff. \n",
    "\n",
    "Ref: https://insidebigdata.com/2014/10/22/ask-data-scientist-bias-vs-variance-tradeoff/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall\n",
    "![title](https://cdn-images-1.medium.com/max/1600/1*Z54JgbS4DUwWSknhDCvNTQ.png)\n",
    "Precision tells how many correct instances for a class are predicted among all the prediction for the class. In other words \"How many selected items are relevant?\"\n",
    "\n",
    "$\\text{precision} = \\frac{tp}{tp+fp} = \\frac{\\text{true positive}}{\\text{Total predicted positive}}$\n",
    "\n",
    "           \n",
    "\n",
    "Recall tells the precentage of relevant items predicted from the actual list of relevant items. In other words, \"How many relevant items are selected?\"\n",
    "\n",
    "$\\text{recall}=\\text{sensitivity} = \\frac{tp}{tp+fn}=\\frac{\\text{true positive}}{\\text{Total actual positive}}$\n",
    "\n",
    "$\\text{specificity} = \\frac{tn}{fn+fp}$\n",
    "\n",
    "Specificity tells how good a test is at avoiding false alams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score\n",
    "F1 score is a measure of a test's accuracy. It considers both precision $p$ and recall $r$ of the test to compute the score. \n",
    "F1 score is given by, \n",
    "\\begin{equation}\n",
    "F1 = 2.\\frac{\\text{precision}.\\text{recall}}{\\text{precision}+\\text{recall}}\n",
    "\\end{equation}\n",
    "\n",
    "F1 is the harmonic mean of precision and recall. In the event of unbalanced datasets F1 score might be a better measure to use if one needs to seek a balance between Precision and Recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harmonic mean\n",
    "The harmonic mean (sometimes called as the subcontrary mean) is one of the several kinds of average, and in particular one of the Pythagorean means. Typically, it is appropriate for situations when the average of *rates* is desired. \n",
    "The harmonic mean $H$ of the positive real numbers $x_1,x_2,....,x_n$ is defined to be \n",
    "\\begin{equation}\n",
    "H = \\frac{n}{\\frac{1}{x_1}+\\frac{1}{x_2}+......+\\frac{1}{x_n}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type I and Type II Errors\n",
    "Type I Error: \n",
    "* Rejecting Null Hypothesis but it is *True*.\n",
    "Type II Error: \n",
    "* Null Hypothesis if *False* and it is *not rejected*. Potentially high risk. \n",
    "\n",
    "False Positives FP --> Type I Error\n",
    "False Negative FN --> Type II Error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC and ROC Curves\n",
    "AUC - Area under the curve\n",
    "ROC - Receive Operator Characteristic \n",
    "ROC curve is a performance measurement for classification problem at various threshold settings. It the curve with false positive rate in x-axis and true positive rate in the y-axis.  \n",
    "\n",
    "True Positive Rate is RECALL. \n",
    "\n",
    "For an excellent model, the AUC has to be closer to 1. \n",
    "AUC gives a measure of separability between good and bad samples. \n",
    "\n",
    "For a multi-class model, one can use one vs All methodology to get many ROC curves. \n",
    "\n",
    "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-supervised learning\n",
    "Semi-supervised learning is class of ML algorithm which fallse between supervised and unsupervised learning techniques. This is because, the dataset consists of a small amount of labeled and a larget quantity of unlabeled data. \n",
    "\n",
    "### Self-supervised learning: \n",
    "Refers to the approach where the labels can be generated automatically. \n",
    "Eg. Autoencoders\n",
    "\n",
    "### Weakly supervised learning: \n",
    "Bootstrapping, also called self-training, is a form of learning that is designed to use even less training examples, therefore sometimes called weakly-supervised. Bootstrapping starts with a few training examples, trains a classifier, and uses thought-to-be positive examples as yielded by this classifier for retraining. As the set of training examples grows, the classifier improves, provided that not too many negative examples are misclassified as positive, which could lead to deterioration of performance.\n",
    "\n",
    "\n",
    "Ref: https://stackoverflow.com/questions/18944805/what-is-weakly-supervised-learning-bootstrapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours vs K-Means\n",
    "K-means is a clustering algorithm that tries to partition a set of points into K sets (clusters) such that the points in each cluster tend to be near each other. It is unsupervised because the points have no external classification.\n",
    "\n",
    "K-nearest neighbors is a classification (or regression) algorithm that in order to determine the classification of a point, combines the classification of the K nearest points. It is supervised because you are trying to classify a point based on the known classification of other points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "It is a classification methodology which uses Bayes Theorem with an assumption that the individual input features are independent to each other and bear no cross-correlation. This basic assumption is the reason why naive bayes is called \"naive\". \n",
    "\n",
    "Bayes theorem says \n",
    "\\begin{align}\n",
    "P(c|x) &= \\frac{P(x|c).P(c)}{P(x)} \\\\\n",
    "\\text{Posterior Probability} &= \\frac{\\text{Likelihood.Prior probability class}}{\\text{Prior probability prediction}}\n",
    "\\end{align}\n",
    "\n",
    "##### Pros:\n",
    "* Easy and fast to predict class of test data set, \n",
    "* If the assumption of independence holds, performs even better than other classification methods, \n",
    "* performs well for categorical variables compared to numerical variables. For numerical variable, normal distribution is assumed. \n",
    "\n",
    "##### Cons: \n",
    "* Has problem generalizing against novel examples.\n",
    "* In real life, independent variables do not occur. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
