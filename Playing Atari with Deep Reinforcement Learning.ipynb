{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Reinforcement Learning\n",
    "\"Playing Atari with Deep Reinforcement Learning\" is a breakthrough paper from DeepMind Technologies where a convolution neural network was modeled by training with a variang of Q-learning (Reinforcement Learning Technique). The model is used to learn Atari 2600 games using only input image or screenshot of the game in pixels to output a valud function estimating future rewards. The authors show that their Deep Q-Learning model (DQN) outperforms previous approaches of RL to learn Atari 2600 games on six of the seven games and even performs better than a human expert on three such games. \n",
    "\n",
    "Paper: Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\" arXiv preprint arXiv:1312.5602 (2013).\n",
    "arxiv: https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definitions\n",
    "\n",
    "* **Agent**: An agent is the machine which takes actions. A robot move to pick up an object or a drone making a delivery etc. \n",
    "* **Action (A)**: A is the set of all possible actions the agent can make at a given state $s$. \n",
    "* **Discount factor** : The discount factor is multiplied by future rewards as discovered by agents in order to dampen these rewards effect on the current agents selection of action, \n",
    "* **Environment**: The world where the agent moves. \n",
    "* **State (S)** : The definition of the situation where the agent finds itself in the environment. \n",
    "* **Reward (R)**: Feedback acquired by observing the environment after the agent has performed an action. This indicates if the agents action were a success or a failure. Depending on the problem, rewards can be immediate or delayed. \n",
    "* **Policy ($\\Pi$)**: Policy is the strategy the agent employs to move in the environment with action $a$ and transition to the next state $s'$ from its current state $s$. \n",
    "* **Value (V)**: The expected long-term return with discount. $V_\\Pi(s)$ is defined as the expected long-term return of the current state under the policy $\\Pi$. \n",
    "* **Q-Value or Action-Value (Q)**: Q-value is similar to value in that it also considers the action $a$. $Q_\\Pi(s,a)$ refers to the long-term return of the current state $s$, taking action $a$ under policy $\\Pi$. Q maps state-action pairs to rewards.\n",
    "* **Trajectory** : A sequence of states and actions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quote directly from [1]\n",
    "\n",
    "Environments are functions that transform an action taken in the current state into the next state and a reward; agents are functions that transform the new state and reward into the next action. We can know the agent's function, but we cannot know the function of the environment. It is ablack box where we only see the inputs and outputs. Reinforcement learning represents an agent's attempt to approximate the environments function, such that we can sent actions into the black-box environment that maximize the rewards it spits out.\n",
    "\n",
    "##### Markov Decision Process (MDP)\n",
    "\n",
    "The setup of MDP with an added constraint is that the probability of reaching the next state is purely dependent on the current state. The goal is to maximize the utility or in other words learn a policy function $\\Pi$ that maps states to actions.\n",
    "\n",
    "If one considers a mobile robot as an agent trying to navigate a dynamic environment, reinforcement learning is the problem faced by the robot to learn a strategy or behavior through its trial-and-error interactions with the environment. Given a state and the different actions that the agent can execute, the problem is to find the right strategy or policy or state-action-pairs which maximize a long time reward.\n",
    "\n",
    "Formally, the model consists of [2],\n",
    "* a discrete set of environment states, $S$,\n",
    "* a discrete set of agent actions, $A$; and\n",
    "* a set of scalar reinforcement signals; typically {0,1}, or the real numbers.\n",
    "* a transitional probability. (Non deterministic) Probability of reaching the next state $s'$ from state $s$ when executing an action $a$. The inherent stochasticity associated with the agents movement and the environment is encapsulated by the transitional probability.\n",
    "\n",
    "The agents job is to find a policy $\\Pi$ which maps the input preceived to corresponding actions such that it maximizes some long-term measure of positive reinforcement. If we make the standard assumption that the future rewards are discounted by a factor of $\\gamma$ per timestep, then the future discounted long-term reward at a specific time is given by, \n",
    "\\begin{equation}\n",
    "R_t = \\sum_{i=1}^T \\gamma^{i-1}r_{i}\n",
    "\\end{equation}\n",
    "where $T$ is the time-step at which the task terminates. \n",
    "\n",
    "Value-iteration and policy-iteration are two important algorithms that solve the above mentioned Markov Decision Process (MDP). \n",
    "\n",
    "##### Value-iteration\n",
    "The value function $V(s)$ is an estimate of how good the current state is and for the agent to be in. If the agent currently follows a policy $\\Pi$, the corresponding value function for this policy is given by, \n",
    "\\begin{equation}\n",
    "V^\\Pi(s) = E[\\sum_{i=1}^T \\gamma^{i-1} r_i], \\forall s \\epsilon S\n",
    "\\end{equation}\n",
    "\n",
    "From all the available value functions, there exists one optimal value function corresponding to a specific policy that has the maximum value. It is given by, \n",
    "\\begin{equation}\n",
    "V^*(s)=\\max_\\Pi V^\\Pi(s), \\forall s \\epsilon S\n",
    "\\end{equation}\n",
    "The optimal policy $\\Pi^*$ is the policy that corresponds to optimal value function, \n",
    "\\begin{equation}\n",
    "\\Pi^* = \\text{arg}\\max_\\Pi V^\\Pi(s), \\forall s \\epsilon S\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In RL, the state action pair is also denoted as a Q function. The Q-function thereby returns a real value. \n",
    "\n",
    "$Q:S \\times A \\rightarrow R$\n",
    "\"The optimal Q function $Q^*(s,a)$ means the expected total reward received by an agent starting in $s$ and picks an action $a$, then will behave optimally afterwards. Since $V^*(s)$ is the maximum expected total reward when starting from state $s$, it will be the maximum of $Q^*(s,a)$ over all possible actions.\" [4]\n",
    "\n",
    "Therefore, the relationship between $Q^*(s,a)$ and $V^*(s)$ is given by, \n",
    "\n",
    "$V^*(s) = \\max_a Q^*(s,a), \\forall s \\epsilon S$\n",
    "So, given the optimal Q-function $Q^*(s,a)$, the optimal policy is obtained by choosing the action $a$ that gives the maximum $Q^*(s,a)$ for state $s$.\n",
    "\n",
    "$\\Pi^*(s)=\\text{arg}\\max_a Q^*(s,a), \\forall s \\epsilon S$\n",
    "\n",
    "This optimal value function follows an important identity known as **Bellmann equation**, which uses a dynamic programming paradigm and provides a recursive definition for the optimal Q-function. \n",
    "The intuition is as follows:\n",
    "\n",
    "\"if the optimal value Q^*(s',a') of the sequence $s'$ at the next time-step was known for all possible actions $a'$, then the optimal strategy is to selection the action $a'$ maximizing the expected value of $r + \\gamma  Q^*(s,a)$ given by, \"\n",
    "\n",
    "\\begin{align}\n",
    "Q^*(s,a) &= R(s,a)+\\gamma E_{s'}[V^*(s')] \\\\\n",
    "Q^*(s,a) &= R(s,a)+\\gamma \\sum_{s'\\epsilon S}p(s'|s,a)V^*(s') \\\\\n",
    "\\text{since,}\n",
    "V^*(S) &= \\max_a Q^*(s,a)\\\\\n",
    "V^*(S) &= \\max_a [R(s,a)+\\gamma \\sum_{s'\\epsilon S}p(s'|s,a)V^*(s')]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "Value Iteration computes the optimal state value function by iteratively improving the estimate of $V(s)$.\n",
    "The pseudo-code is given by, \n",
    "\n",
    "* Initialize V(s) to arbitrary values, \n",
    "* Repeat\n",
    "    * For all $s\\epsilon S$\n",
    "        * For all $a \\epsilon A$\n",
    "            * $Q(s,a) \\leftarrow E[r|s,a] + \\gamma \\sum_{s'\\epsilon S} P(s'|s,a)V(s')$\n",
    "        * $V(s) \\leftarrow \\max_a Q(s,a)$\n",
    "* Until $V(s)$ converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Iteration\n",
    "While value iteration iterates on the value function, policy iteration iterates over set of policies to determine the optimal policy. \n",
    "\n",
    "* Intialize a policy $\\Pi'$ arbitrarily, \n",
    "* Repeat\n",
    "    * $\\Pi \\leftarrow \\Pi'$\n",
    "        * Compute the values using $\\Pi$ by solving the linear equations\n",
    "            * $V^\\Pi (s) = E[r|s,\\Pi(s)]+\\gamma \\sum_{s'\\epsilon S}P(s'|s,\\Pi(s))V^\\Pi(s')$\n",
    "        * Improve the policy at each state\n",
    "        * $\\Pi'(s) \\leftarrow \\text{arg}\\max_a (E[r|s,a] + \\gamma \\max_{s' \\epsilon S} P(s'|s,a)V^\\Pi(s))$\n",
    "* Until $\\Pi = \\Pi'$    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning\n",
    "When the agent does not have any apriori knowledge about the state-transition and the reward models, Q-learning provides a methodology to learn from the set of interactions experienced by the agent with the environment. There are two categories of Q-learning namely, \n",
    "* Model-based learning: Here, the agent interacts with the environment, collects experiences and then tries to approximate the state transition and reward models. Once done, the model is learned and then further explored using value-iteration or policy-iteration to find an optimal policy, \n",
    "* Model-free learning: Here, the agent does not learn the explicit models of the environment state transition and reward functions, but derives an optimal policy directly from the interactions with the environment. \n",
    "\n",
    "The basic idea of Q-learning is to approximate the state-action pairs that we observe. This is called Time-Difference Learning (TD Learning). \n",
    "\\begin{align}\n",
    "Q(s,a) &= (1-\\alpha)Q(s,a)+\\alpha Q_{obs}(s,a) where,\\\\\n",
    "Q_{obs}(s,a) &= r(s,a) + \\gamma \\max_{a'}Q(s',a')\n",
    "\\end{align}\n",
    "$\\alpha$ is the learning rate. The $Q(s,a)$ table is initialized randomly. The agent starts by interacting with the environment, and upon each interaction it discovers or observes new set of rewards $r(s,a)$ and state transition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration with function Approximation\n",
    "In value iteration, the updates to the value function is performed until convergence. This is feasible if the subset of states are small but, normally the number of state space is very large where iterating over the entire set of feasible states becomes extremely impractical. For example, in a 9x9 Go game, according to [5], there are $|S|=10^{38}$ states and $|A|=81$ actions. The solution to handle this situation is to obtain *features* and represent states with a function of those features. A linear function representation would look the following, \n",
    "\n",
    "$V(s) = \\theta_0 \\cdot 1 + \\theta_1 \\phi_1(s) + \\dots + \\theta_n \\phi_n(s) = \\theta^T \\phi(s)$\n",
    "$\\theta_i$ are the weights with $theta_0$ the bias term. The function approximation thus drastically reduces the size of the state space from $|S|$ into $|\\Theta|$ where $\\Theta$ is the domain for $\\theta \\epsilon R^n$. Instead of determining $V_{i+1}^*(s)$ for all $s$, each iteration the goal is to update the weights $\\theta_i$.\n",
    "\n",
    "The methodology is to reformulate the learning of the weights through a supervised learning problem. Nevertheless, one would still need the states to generate the data. Hence, instead of using all the states, only a small subset of the states is used. The algorithm thus would look where at every iteration $i$ with current weight vector $\\theta^{(i)}$, a small subset of states $S*\\epsilon S$ are sampled and an official one-step Bellmann backup updates are performed as, \n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{V}_{i+1}(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s')+\\gamma \\hat V_{\\theta^{(i)}} (s')],\\\\\n",
    "\\text{Then the next set of weights:}\\\\\n",
    "\\theta^{(i+1)} = \\text{arg}_\\theta \\min \\sum_{s\\epsilon S'}(V_{\\theta^{(i+1)}}(s)-\\bar V_{i+1} (s))^2\n",
    "\\end{equation}\n",
    "\n",
    "Stochastic Gradient Descent is typically used to learn the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning with Function Approximation\n",
    "For Q-Learning, in addition to the states we also need to use the action $a$ because of the fact,\n",
    "\n",
    "$\\Pi(s) = \\text{arg}_a \\max Q^*(s,a)$\n",
    "\n",
    "To use Q-values with function approximation, we need to use features that are a function of both states and actions. Representing them as a linear function would look like the following, \n",
    "\n",
    "$Q(s,a)=\\theta_0 \\cdot 1+\\theta_1 \\phi_1(s,a) + \\dots + \\theta_n \\phi_n(s,a) = \\theta^T \\phi(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Least Squares Justification for Q-Learning [5]\n",
    "Recall the Q-Learning update without function approximation, \n",
    "\n",
    "$Q(s,a) \\leftarrow (1-\\alpha)Q(s,a)+\\alpha\\underbrace{[R(s,a,s')+\\gamma \\max_{a'}Q(s',a')]}_\\text{sample}$\n",
    "\n",
    "In order to learn this with weight representation, we need a loss function. Hence, as mentioned before we sample a subset of states and compute the \"target\" value as $Q^+(s,a)$ to obtain, \n",
    "\n",
    "$Q^+(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s')+\\gamma \\max_{a'}Q^+(s',a')]$\n",
    "\n",
    "The idea is that averaged over the entire run, this subsample set will average out to the true value. \n",
    "\n",
    "The loss function is thus defined as, \n",
    "\n",
    "$L(\\theta^{i}) = \\frac{1}{2}(Q^+(s,a)-Q(s,a))^2=\\frac{1}{2}(Q^+(s,a)-\\phi(s,a)^T\\theta^{(i)})^2$.\n",
    "\n",
    "The gradient update for a step size $\\alpha$ would thus look like, \n",
    "\n",
    "\\begin{align}\n",
    "\\theta^{(i+1)} &= \\theta^{(i)}-\\alpha \\nabla L(\\theta^{(i)})\\\\\n",
    "               &= \\theta^{(i)}-\\alpha \\nabla \\frac{1}{2}[Q^+(s,a)-\\phi(s,a)^T \\theta^{(i)}]^2\\\\\n",
    "               &= \\theta^{(i)}-\\alpha(Q^+(s,a)-\\phi(s,a)^T \\theta^{(i)})\\cdot \\phi(s,a)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The authors [0] learn the weights $\\theta$ using a Deep Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Reinforcement Learning\n",
    "The goal of the paper is to connect RL algorithm to a deep neural network which works in an end-to-end fashion where the input are RGB images and efficiently process the training data using SGD updates. In contrast to TD-Gammon where the value function estimates were learned directly from on-policy samples of experience, $s_t,a_t,r_t,s_{t+1},a_{t+1}$, drawn from the algorithm's interaction with its environment. In contrast to TD-Gammon the authors employ a technique known as *experience replay* where the agent's experiences at each time-step, $e_t=(s_t,a_t,r_t,s_{t+1})$ are stored in a data-set $D=e_1,\\dots,e_N$, pooled over many episodes into a *replay memory*. In the inner loop of the algorithm, Q-Learning updates are performed to samples of experiences drawn at random from the replay memory. After performing experience replay, the agend selects and executes an action acoording to an $\\epsilon$-greedy policy. Q-function works on fixed length representation of histories produced by a function $\\phi$. Figure below shows the full algorithm. \n",
    "\n",
    "\n",
    "![DQNAlgorithm](https://cdn-images-1.medium.com/max/1600/1*nb61CxDTTAWR1EJnbCl1cA.png)\n",
    "\n",
    "According to the autho the approach has several advantages namely,\n",
    "\n",
    "* Greater data efficiency from each step of experience used for many weight updates, \n",
    "* Learning from random sample of experiences eliminates learning from heavily correlated samples occuring consecutively,\n",
    "* By using experience replay, the behavior distribution is averaged over many of its previous states, smoothing out learning and avoiding oscillations or divergence in the parameters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "RGB Atari frames of size 210x160 pixels were first converted to gray-scale and then downsampled and cropped down to 84x84 pixels. \n",
    "The input to the neural network is thus 84x84x4 image produced by $\\phi$. The first hidden layer convolves 16 $8x8$ filters with stride 4 with the input images and applies a rectifier nonlinearity. The second hidden layer convolves 32 $4x4$ filters with stride 2, again followed by ReLU. The final hidden layer is fully-connected and consists of 256 rectifier units. The output layer is a fully connected linear layer with single output for each valid action. The number of valid actions varied between 4 and 18 on the games. This network is called as ***Deep Q-Networks (DQN)***.\n",
    "\n",
    "Games played: \n",
    "* Beam Rider, \n",
    "* Brakeout,\n",
    "* Enduro, \n",
    "* Pong, \n",
    "* Q*bert,\n",
    "* Seaquest and \n",
    "* Space Invaders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "[0] Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\" arXiv preprint arXiv:1312.5602 (2013).\n",
    " \n",
    "[1] https://skymind.ai/wiki/deep-reinforcement-learning\n",
    "\n",
    "[2] Kaelbling, Leslie Pack, Michael L. Littman, and Andrew W. Moore. \"Reinforcement learning: A survey.\" Journal of artificial intelligence research 4 (1996): 237-285.\n",
    "\n",
    "[3] https://danieltakeshi.github.io/2016/10/31/going-deeper-into-reinforcement-learning-understanding-q-learning-and-linear-function-approximation/ \n",
    "\n",
    "[4] https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa\n",
    "\n",
    "[5] http://research.cs.rutgers.edu/~thomaswa/pub/Geramifard13Tutorial.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
